
[DEFAULT]
# ログファイル名
logger = log
# 用いるGPU 番号
cuda = 0
    
use_segment = 0
# エポック数
n_epoch = 10
# 分割サイズ (データをsplit_sizeフレームごとに分割）
# split_size = 500000   #  では、1エポックで5h以上かかる。
# split_size = 125000   #  でも、1エポックで30min.以上かかる。

# split_size = 60000    #   250000が割り切れないとだめだよ。このsplit_sizeはダメ
# split_size = 50000    #   250000/5
# split_size = 40000    #   160000/4  2264.48 seconds  
# split_size = 35000    #   140000/4
# split_size = 100000   # 2_000_000/10/2 1 epoch 3min. (data_length= 100000)  --> 4min. (data_length= 125000) 
# split_size   = 75000　# 1_500_000/10/2
# split_size = 62500　  # 1_250_000/10/2　1 epoch 2210.19 seconds sec.  # 400 epoch で、15 hou   250000/4
# split_size = 31250    #   1 epoch 177 sec.    250000/8   125000
split_size = 15625    #   1 epoch 128 sec. 
#     で、斜め線が少し見えるが、縦線優位    1 epoch 2min.

# バッチサイズ (学習の1回の更新で使う分割データの個数)
batch_size = 16 # 4 # 32だとメモリーエラーをまれに起こすケースがある # 64 # 32
# 入力次元数（ニューロンの数）  
dim_input = 101
# トレインに使うデータの割合  train(50%) vali(20%) test(30%) for pre-training within training data.
train_rate = 0.5
# バリデーション
valid_rate = 0.2
dropout=0.1
learning_rate = 0.00003  # 5  #   3 # 5 # 3 # 0.0001




[path]
base = /n/work1/gshibata/src/spike_tnb_200424
spike = %(base)s/data/split_spikes.npy
result = %(base)s/data/result

# データファイル
# 結果を入れるディレクトリ

[lstm]
n_lstm_hidden = 128


[lstm_stack]
gamma = 0.0
# Focal lossの重み係数。gamma=0の場合はbinary cross entropyに一致する。
n_layer_list = 1,2,3
# LSTMの隠れ層の数（リスト）。
n_hidden_list = 128,64,256
# この場合は、（入力層->）ユニット数128次元のLSTM 1層、64次元のLSTM 2層、256次元のLSTM 3層（->線形層、softmax->出力層）



[transformer]
gamma = 0.0 # ログファイル名
logger = log
# Focal lossの重み係数。gamma=0の場合はbinary cross entropyに一致する。
n_layer_list = 1,2,3
# Transformerの隠れ層の数（リスト）。
n_hidden_list = 128,64,128
# この場合は、（入力層->）ユニット数128次元のTransformer 1層、64次元のTransformer 2層、256次元のTransformer 3層（->線形層、softmax->出力層）
ntoken = 5000
n_heads = 16
d_model=128
alpha=2.0
threshold_spike_counts = 0.8

[scinet]
gamma = 0.0 # ログファイル名
logger = log
# Focal lossの重み係数。gamma=0の場合はbinary cross entropyに一致する。
n_layer_list = 1,2,3,4
# Transformerの隠れ層の数（リスト）。
n_hidden_list = 128,128,128,128
# この場合は、（入力層->）ユニット数128次元のTransformer 1層、64次元のTransformer 2層、256次元のTransformer 3層（->線形層、softmax->出力層）
d_model=128
alpha=2.0
threshold_spike_counts = 0.7

